{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Set Working directory"],"metadata":{"id":"Ds7xaLopy7v1"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TwtJfz4KpVM_","executionInfo":{"status":"ok","timestamp":1648077374540,"user_tz":0,"elapsed":36840,"user":{"displayName":"Evern Joshua","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16916867645640256262"}},"outputId":"a52647aa-1560-4811-9640-2b8111805bac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","os.chdir('/content/drive/MyDrive/project') # main working directory"],"metadata":{"id":"FiAzUtjGeX7e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dir='segmented/' # path of the training set"],"metadata":{"id":"XHrWYQbji9Qt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name='best_model.h5'"],"metadata":{"id":"Ex27NABxjQr_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# importing libraries"],"metadata":{"id":"9bnudwSUy3Ze"}},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten, BatchNormalization\n","from sklearn.metrics import classification_report, accuracy_score\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.preprocessing.image import load_img\n","from keras.callbacks import ModelCheckpoint\n","import numpy as np\n","import warnings\n","from sklearn.exceptions import UndefinedMetricWarning\n","warnings.filterwarnings('ignore', category=UndefinedMetricWarning)\n","import tensorflow as tf\n","from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n","import matplotlib.pyplot as plt"],"metadata":{"id":"hQl2ebHYlKkW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Function to build model, trining model, plot training and test model"],"metadata":{"id":"JU4n4xGRymcJ"}},{"cell_type":"code","source":["def model_build():\n","  model=Sequential()\n","  model.add(tf.keras.Input(shape=(64,64,)))\n","  model.add(Flatten())\n","  model.add(Dense(256,activation='relu'))\n","  model.add(Dense(256,activation='relu'))\n","  model.add(Dense(256,activation='relu'))\n","  model.add(Dense(256,activation='relu'))\n","  model.add(Dense(256,activation='relu'))\n","  model.add(Dense(256,activation='relu'))\n","  model.add(Dense(256,activation='relu'))\n","  model.add(Dense(256,activation='relu'))\n","  model.add(Dense(256,activation='relu'))\n","  model.add(Dense(45,activation='softmax'))\n","  model.compile(optimizer=Adam(learning_rate=0.001, decay=1e-6), \n","                loss='binary_crossentropy', \n","                metrics=['accuracy'])\n","  return model\n","\n","def model_fit(model,model_path):\n","  callbacks=[\n","             ModelCheckpoint(filepath=model_path,\n","                              save_best_only=True,\n","                              verbose=1,\n","                              mode='min',\n","                              moniter='val_loss')\n","             ]\n","  history=model.fit(train_set,\n","                    validation_data=val_set,\n","                    epochs = 300,\n","                    verbose = 1,\n","                    callbacks=callbacks)\n","  return history\n","  \n","def plot_training(history):\n","  plt.figure(figsize=(14,5))\n","  plt.subplot(1,2,2)\n","  plt.plot(history.history['accuracy'])\n","  plt.plot(history.history['val_accuracy'])\n","  plt.title('Model Accuracy')\n","  plt.xlabel('Epochs')\n","  plt.ylabel('Accuracy')\n","  plt.legend(['train', 'test'], loc='upper left')\n","\n","  plt.subplot(1,2,1)\n","  plt.plot(history.history['loss'])\n","  plt.plot(history.history['val_loss'])\n","  plt.title('model Loss')\n","  plt.xlabel('Epochs')\n","  plt.ylabel('Loss')\n","  plt.legend(['train', 'test'], loc='upper left')\n","  plt.show()\n","\n","def test_model(model,test_set):\n","  y_pred = model.predict(test_set)\n","  y_pred = np.argmax(y_pred, axis=1)\n","  print('Model classification accurcy:',np.round(accuracy_score(test_set.classes,y_pred),2))\n","  class_labels = test_set.class_indices\n","  class_labels = {v:k for k,v in class_labels.items()}\n","  print('Classification Report')\n","  target_names = list(class_labels.values())\n","  print(classification_report(test_set.classes, y_pred, target_names=target_names))\n","\n"],"metadata":{"id":"oqoDQ5GKunKi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Set directories and shape of input image"],"metadata":{"id":"pHn1-x6nygkJ"}},{"cell_type":"code","source":["# test_dir='output/val/' # path of the testing set\n","target_size=(64,64)"],"metadata":{"id":"VH4_Fpchl6eT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualize Dataset"],"metadata":{"id":"SQBgc3EpydUr"}},{"cell_type":"code","source":["plt.figure(figsize=(50,50))\n","for i, clas in enumerate(os.listdir(train_dir)):\n","    plt.subplot(1,len(os.listdir(train_dir)),i+1)\n","    class_dir=train_dir+clas\n","    total_pic=len(os.listdir(class_dir))\n","    pic_num=np.random.randint(total_pic)\n","    img = load_img((train_dir + clas +'/'+ os.listdir(train_dir + clas)[pic_num]))\n","    plt.imshow(img)\n","    plt.title(clas)\n","    plt.axis('off')\n","plt.show()"],"metadata":{"id":"YdCxL1p5lmJ_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Build Dataset into keras environment"],"metadata":{"id":"wxvJhGVlyS_7"}},{"cell_type":"code","source":["datagen = ImageDataGenerator(validation_split=0.1)\n","\n","train_set = datagen.flow_from_directory(train_dir,\n","                                          batch_size=128,\n","                                          target_size=target_size,\n","                                          color_mode='grayscale',\n","                                          class_mode='categorical',\n","                                          subset='training',\n","                                          shuffle=False)\n","\n","val_set = datagen.flow_from_directory(train_dir,\n","                                      batch_size=128,\n","                                      target_size=target_size,\n","                                      color_mode='grayscale',\n","                                      class_mode='categorical',\n","                                      subset='validation',\n","                                      shuffle=False)\n","\n","# datagen = ImageDataGenerator()\n","\n","# test_set = datagen.flow_from_directory(test_dir,\n","#                                       batch_size=64,\n","#                                       target_size=target_size,\n","#                                       color_mode='grayscale',\n","#                                       class_mode='categorical',\n","#                                        shuffle=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ePVI7F8OmD-U","executionInfo":{"status":"ok","timestamp":1648077464025,"user_tz":0,"elapsed":357,"user":{"displayName":"Evern Joshua","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16916867645640256262"}},"outputId":"827a6315-2a17-41b3-f2b6-c26f5b05b403"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 415 images belonging to 45 classes.\n","Found 45 images belonging to 45 classes.\n"]}]},{"cell_type":"code","source":["train_set.class_indices"],"metadata":{"id":"3g1dCWVf4___"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Resnet 50"],"metadata":{"id":"qS_bP55QyQ_R"}},{"cell_type":"code","source":["model = model_build()\n","model.build(input_shape=(32,32,))\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jSyH4Yqboacu","executionInfo":{"status":"ok","timestamp":1648077478951,"user_tz":0,"elapsed":3400,"user":{"displayName":"Evern Joshua","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16916867645640256262"}},"outputId":"0d01c749-3355-442d-f7e2-6fb5e837b406"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," flatten (Flatten)           (None, 4096)              0         \n","                                                                 \n"," dense (Dense)               (None, 256)               1048832   \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               65792     \n","                                                                 \n"," dense_2 (Dense)             (None, 256)               65792     \n","                                                                 \n"," dense_3 (Dense)             (None, 256)               65792     \n","                                                                 \n"," dense_4 (Dense)             (None, 256)               65792     \n","                                                                 \n"," dense_5 (Dense)             (None, 256)               65792     \n","                                                                 \n"," dense_6 (Dense)             (None, 256)               65792     \n","                                                                 \n"," dense_7 (Dense)             (None, 256)               65792     \n","                                                                 \n"," dense_8 (Dense)             (None, 256)               65792     \n","                                                                 \n"," dense_9 (Dense)             (None, 45)                11565     \n","                                                                 \n","=================================================================\n","Total params: 1,586,733\n","Trainable params: 1,586,733\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["history=model_fit(model,model_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HYj54M4K04yw","executionInfo":{"status":"ok","timestamp":1648077772890,"user_tz":0,"elapsed":289558,"user":{"displayName":"Evern Joshua","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16916867645640256262"}},"outputId":"8a22fab7-09ff-40c4-9cb2-21e4e2766593"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/300\n","WARNING:tensorflow:Model was constructed with shape (None, 64, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, 64, 64), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape (None, None, None, None).\n","WARNING:tensorflow:Model was constructed with shape (None, 64, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, 64, 64), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape (None, None, None, None).\n","4/4 [==============================] - ETA: 0s - loss: 1.5559 - accuracy: 0.0096 WARNING:tensorflow:Model was constructed with shape (None, 64, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, 64, 64), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape (None, None, None, None).\n","\n","Epoch 1: val_loss improved from inf to 0.52109, saving model to best_model.h5\n","4/4 [==============================] - 88s 28s/step - loss: 1.5559 - accuracy: 0.0096 - val_loss: 0.5211 - val_accuracy: 0.0222\n","Epoch 2/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.4326 - accuracy: 0.0286\n","Epoch 2: val_loss improved from 0.52109 to 0.36331, saving model to best_model.h5\n","4/4 [==============================] - 1s 164ms/step - loss: 0.4403 - accuracy: 0.0265 - val_loss: 0.3633 - val_accuracy: 0.0222\n","Epoch 3/300\n","4/4 [==============================] - ETA: 0s - loss: 0.3691 - accuracy: 0.0193\n","Epoch 3: val_loss improved from 0.36331 to 0.24169, saving model to best_model.h5\n","4/4 [==============================] - 1s 200ms/step - loss: 0.3691 - accuracy: 0.0193 - val_loss: 0.2417 - val_accuracy: 0.0222\n","Epoch 4/300\n","4/4 [==============================] - ETA: 0s - loss: 0.2211 - accuracy: 0.0048\n","Epoch 4: val_loss improved from 0.24169 to 0.17058, saving model to best_model.h5\n","4/4 [==============================] - 1s 195ms/step - loss: 0.2211 - accuracy: 0.0048 - val_loss: 0.1706 - val_accuracy: 0.0444\n","Epoch 5/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1931 - accuracy: 0.0145\n","Epoch 5: val_loss improved from 0.17058 to 0.15288, saving model to best_model.h5\n","4/4 [==============================] - 1s 166ms/step - loss: 0.1931 - accuracy: 0.0145 - val_loss: 0.1529 - val_accuracy: 0.0222\n","Epoch 6/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.1595 - accuracy: 0.0000e+00\n","Epoch 6: val_loss improved from 0.15288 to 0.14629, saving model to best_model.h5\n","4/4 [==============================] - 1s 167ms/step - loss: 0.1580 - accuracy: 0.0000e+00 - val_loss: 0.1463 - val_accuracy: 0.0000e+00\n","Epoch 7/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1602 - accuracy: 0.0217\n","Epoch 7: val_loss improved from 0.14629 to 0.14359, saving model to best_model.h5\n","4/4 [==============================] - 1s 170ms/step - loss: 0.1602 - accuracy: 0.0217 - val_loss: 0.1436 - val_accuracy: 0.0222\n","Epoch 8/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1407 - accuracy: 0.0048\n","Epoch 8: val_loss improved from 0.14359 to 0.13280, saving model to best_model.h5\n","4/4 [==============================] - 1s 173ms/step - loss: 0.1407 - accuracy: 0.0048 - val_loss: 0.1328 - val_accuracy: 0.0667\n","Epoch 9/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1394 - accuracy: 0.0145\n","Epoch 9: val_loss improved from 0.13280 to 0.13105, saving model to best_model.h5\n","4/4 [==============================] - 1s 194ms/step - loss: 0.1394 - accuracy: 0.0145 - val_loss: 0.1310 - val_accuracy: 0.0222\n","Epoch 10/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1335 - accuracy: 0.0000e+00\n","Epoch 10: val_loss improved from 0.13105 to 0.12626, saving model to best_model.h5\n","4/4 [==============================] - 1s 163ms/step - loss: 0.1335 - accuracy: 0.0000e+00 - val_loss: 0.1263 - val_accuracy: 0.0222\n","Epoch 11/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1298 - accuracy: 0.0217\n","Epoch 11: val_loss improved from 0.12626 to 0.12215, saving model to best_model.h5\n","4/4 [==============================] - 1s 196ms/step - loss: 0.1298 - accuracy: 0.0217 - val_loss: 0.1221 - val_accuracy: 0.0222\n","Epoch 12/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.1247 - accuracy: 0.0208\n","Epoch 12: val_loss improved from 0.12215 to 0.12089, saving model to best_model.h5\n","4/4 [==============================] - 1s 167ms/step - loss: 0.1231 - accuracy: 0.0434 - val_loss: 0.1209 - val_accuracy: 0.0444\n","Epoch 13/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.1234 - accuracy: 0.0000e+00\n","Epoch 13: val_loss did not improve from 0.12089\n","4/4 [==============================] - 1s 123ms/step - loss: 0.1206 - accuracy: 0.0241 - val_loss: 0.1212 - val_accuracy: 0.0222\n","Epoch 14/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1225 - accuracy: 0.0241\n","Epoch 14: val_loss did not improve from 0.12089\n","4/4 [==============================] - 1s 131ms/step - loss: 0.1225 - accuracy: 0.0241 - val_loss: 0.1221 - val_accuracy: 0.0222\n","Epoch 15/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1211 - accuracy: 0.0361\n","Epoch 15: val_loss improved from 0.12089 to 0.11530, saving model to best_model.h5\n","4/4 [==============================] - 1s 205ms/step - loss: 0.1211 - accuracy: 0.0361 - val_loss: 0.1153 - val_accuracy: 0.0222\n","Epoch 16/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1146 - accuracy: 0.0651\n","Epoch 16: val_loss improved from 0.11530 to 0.11385, saving model to best_model.h5\n","4/4 [==============================] - 1s 176ms/step - loss: 0.1146 - accuracy: 0.0651 - val_loss: 0.1139 - val_accuracy: 0.0667\n","Epoch 17/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1155 - accuracy: 0.0554\n","Epoch 17: val_loss improved from 0.11385 to 0.11164, saving model to best_model.h5\n","4/4 [==============================] - 1s 199ms/step - loss: 0.1155 - accuracy: 0.0554 - val_loss: 0.1116 - val_accuracy: 0.0444\n","Epoch 18/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1118 - accuracy: 0.0554\n","Epoch 18: val_loss improved from 0.11164 to 0.11088, saving model to best_model.h5\n","4/4 [==============================] - 1s 194ms/step - loss: 0.1118 - accuracy: 0.0554 - val_loss: 0.1109 - val_accuracy: 0.0444\n","Epoch 19/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.1130 - accuracy: 0.0521\n","Epoch 19: val_loss improved from 0.11088 to 0.10958, saving model to best_model.h5\n","4/4 [==============================] - 1s 163ms/step - loss: 0.1106 - accuracy: 0.0916 - val_loss: 0.1096 - val_accuracy: 0.0444\n","Epoch 20/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.1100 - accuracy: 0.0312\n","Epoch 20: val_loss improved from 0.10958 to 0.10631, saving model to best_model.h5\n","4/4 [==============================] - 1s 167ms/step - loss: 0.1073 - accuracy: 0.0819 - val_loss: 0.1063 - val_accuracy: 0.0444\n","Epoch 21/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1061 - accuracy: 0.0771\n","Epoch 21: val_loss did not improve from 0.10631\n","4/4 [==============================] - 1s 120ms/step - loss: 0.1061 - accuracy: 0.0771 - val_loss: 0.1086 - val_accuracy: 0.0222\n","Epoch 22/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1056 - accuracy: 0.0747\n","Epoch 22: val_loss did not improve from 0.10631\n","4/4 [==============================] - 0s 122ms/step - loss: 0.1056 - accuracy: 0.0747 - val_loss: 0.1093 - val_accuracy: 0.0222\n","Epoch 23/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1055 - accuracy: 0.0651\n","Epoch 23: val_loss improved from 0.10631 to 0.10572, saving model to best_model.h5\n","4/4 [==============================] - 1s 163ms/step - loss: 0.1055 - accuracy: 0.0651 - val_loss: 0.1057 - val_accuracy: 0.0222\n","Epoch 24/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1017 - accuracy: 0.1084\n","Epoch 24: val_loss improved from 0.10572 to 0.10509, saving model to best_model.h5\n","4/4 [==============================] - 1s 174ms/step - loss: 0.1017 - accuracy: 0.1084 - val_loss: 0.1051 - val_accuracy: 0.0444\n","Epoch 25/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0997 - accuracy: 0.1133\n","Epoch 25: val_loss did not improve from 0.10509\n","4/4 [==============================] - 1s 126ms/step - loss: 0.0997 - accuracy: 0.1133 - val_loss: 0.1070 - val_accuracy: 0.0222\n","Epoch 26/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1044 - accuracy: 0.0964\n","Epoch 26: val_loss improved from 0.10509 to 0.10353, saving model to best_model.h5\n","4/4 [==============================] - 1s 168ms/step - loss: 0.1044 - accuracy: 0.0964 - val_loss: 0.1035 - val_accuracy: 0.0444\n","Epoch 27/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0970 - accuracy: 0.1205\n","Epoch 27: val_loss improved from 0.10353 to 0.10117, saving model to best_model.h5\n","4/4 [==============================] - 1s 162ms/step - loss: 0.0970 - accuracy: 0.1205 - val_loss: 0.1012 - val_accuracy: 0.0222\n","Epoch 28/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0966 - accuracy: 0.0771\n","Epoch 28: val_loss did not improve from 0.10117\n","4/4 [==============================] - 1s 123ms/step - loss: 0.0966 - accuracy: 0.0771 - val_loss: 0.1063 - val_accuracy: 0.0444\n","Epoch 29/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0955 - accuracy: 0.1446\n","Epoch 29: val_loss did not improve from 0.10117\n","4/4 [==============================] - 1s 132ms/step - loss: 0.0955 - accuracy: 0.1446 - val_loss: 0.1051 - val_accuracy: 0.0444\n","Epoch 30/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0936 - accuracy: 0.1373\n","Epoch 30: val_loss did not improve from 0.10117\n","4/4 [==============================] - 1s 123ms/step - loss: 0.0936 - accuracy: 0.1373 - val_loss: 0.1036 - val_accuracy: 0.0444\n","Epoch 31/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0942 - accuracy: 0.1562\n","Epoch 31: val_loss did not improve from 0.10117\n","4/4 [==============================] - 1s 131ms/step - loss: 0.0907 - accuracy: 0.2024 - val_loss: 0.1038 - val_accuracy: 0.0222\n","Epoch 32/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0903 - accuracy: 0.1735\n","Epoch 32: val_loss did not improve from 0.10117\n","4/4 [==============================] - 1s 156ms/step - loss: 0.0903 - accuracy: 0.1735 - val_loss: 0.1050 - val_accuracy: 0.0667\n","Epoch 33/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0882 - accuracy: 0.2000\n","Epoch 33: val_loss did not improve from 0.10117\n","4/4 [==============================] - 1s 126ms/step - loss: 0.0882 - accuracy: 0.2000 - val_loss: 0.1029 - val_accuracy: 0.0889\n","Epoch 34/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0945 - accuracy: 0.1590\n","Epoch 34: val_loss did not improve from 0.10117\n","4/4 [==============================] - 1s 131ms/step - loss: 0.0945 - accuracy: 0.1590 - val_loss: 0.1073 - val_accuracy: 0.0222\n","Epoch 35/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0912 - accuracy: 0.1797\n","Epoch 35: val_loss did not improve from 0.10117\n","4/4 [==============================] - 1s 126ms/step - loss: 0.0883 - accuracy: 0.2096 - val_loss: 0.1038 - val_accuracy: 0.1778\n","Epoch 36/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0893 - accuracy: 0.1536\n","Epoch 36: val_loss did not improve from 0.10117\n","4/4 [==============================] - 1s 121ms/step - loss: 0.0859 - accuracy: 0.2024 - val_loss: 0.1075 - val_accuracy: 0.1111\n","Epoch 37/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0874 - accuracy: 0.1901\n","Epoch 37: val_loss improved from 0.10117 to 0.09856, saving model to best_model.h5\n","4/4 [==============================] - 1s 163ms/step - loss: 0.0839 - accuracy: 0.2337 - val_loss: 0.0986 - val_accuracy: 0.1556\n","Epoch 38/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0814 - accuracy: 0.2530\n","Epoch 38: val_loss did not improve from 0.09856\n","4/4 [==============================] - 1s 126ms/step - loss: 0.0814 - accuracy: 0.2530 - val_loss: 0.1016 - val_accuracy: 0.1778\n","Epoch 39/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0852 - accuracy: 0.2024\n","Epoch 39: val_loss did not improve from 0.09856\n","4/4 [==============================] - 1s 158ms/step - loss: 0.0852 - accuracy: 0.2024 - val_loss: 0.1024 - val_accuracy: 0.0889\n","Epoch 40/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0808 - accuracy: 0.2604\n","Epoch 40: val_loss did not improve from 0.09856\n","4/4 [==============================] - 1s 122ms/step - loss: 0.0773 - accuracy: 0.3012 - val_loss: 0.1016 - val_accuracy: 0.2000\n","Epoch 41/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0787 - accuracy: 0.2819\n","Epoch 41: val_loss did not improve from 0.09856\n","4/4 [==============================] - 1s 159ms/step - loss: 0.0787 - accuracy: 0.2819 - val_loss: 0.1010 - val_accuracy: 0.1778\n","Epoch 42/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0867 - accuracy: 0.1849\n","Epoch 42: val_loss did not improve from 0.09856\n","4/4 [==============================] - 1s 126ms/step - loss: 0.0831 - accuracy: 0.2265 - val_loss: 0.1067 - val_accuracy: 0.1333\n","Epoch 43/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0805 - accuracy: 0.2313\n","Epoch 43: val_loss did not improve from 0.09856\n","4/4 [==============================] - 1s 127ms/step - loss: 0.0805 - accuracy: 0.2313 - val_loss: 0.1007 - val_accuracy: 0.1556\n","Epoch 44/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.2819\n","Epoch 44: val_loss improved from 0.09856 to 0.09535, saving model to best_model.h5\n","4/4 [==============================] - 1s 163ms/step - loss: 0.0753 - accuracy: 0.2819 - val_loss: 0.0954 - val_accuracy: 0.2000\n","Epoch 45/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.3349\n","Epoch 45: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 158ms/step - loss: 0.0733 - accuracy: 0.3349 - val_loss: 0.1044 - val_accuracy: 0.2000\n","Epoch 46/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0745 - accuracy: 0.3542\n","Epoch 46: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 126ms/step - loss: 0.0715 - accuracy: 0.3880 - val_loss: 0.0986 - val_accuracy: 0.2667\n","Epoch 47/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.3542\n","Epoch 47: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 167ms/step - loss: 0.0710 - accuracy: 0.3542 - val_loss: 0.1049 - val_accuracy: 0.2000\n","Epoch 48/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0797 - accuracy: 0.3036\n","Epoch 48: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 124ms/step - loss: 0.0797 - accuracy: 0.3036 - val_loss: 0.1057 - val_accuracy: 0.1556\n","Epoch 49/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0799 - accuracy: 0.2675\n","Epoch 49: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 129ms/step - loss: 0.0799 - accuracy: 0.2675 - val_loss: 0.1018 - val_accuracy: 0.2222\n","Epoch 50/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.3542\n","Epoch 50: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 155ms/step - loss: 0.0728 - accuracy: 0.3542 - val_loss: 0.1037 - val_accuracy: 0.2222\n","Epoch 51/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.3157\n","Epoch 51: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 123ms/step - loss: 0.0719 - accuracy: 0.3157 - val_loss: 0.0995 - val_accuracy: 0.2000\n","Epoch 52/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.3831\n","Epoch 52: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 127ms/step - loss: 0.0689 - accuracy: 0.3831 - val_loss: 0.0995 - val_accuracy: 0.2222\n","Epoch 53/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0752 - accuracy: 0.3099\n","Epoch 53: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 130ms/step - loss: 0.0730 - accuracy: 0.3325 - val_loss: 0.1134 - val_accuracy: 0.1556\n","Epoch 54/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0769 - accuracy: 0.2964\n","Epoch 54: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 128ms/step - loss: 0.0769 - accuracy: 0.2964 - val_loss: 0.1003 - val_accuracy: 0.2444\n","Epoch 55/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0719 - accuracy: 0.3438\n","Epoch 55: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 123ms/step - loss: 0.0689 - accuracy: 0.3735 - val_loss: 0.1034 - val_accuracy: 0.2222\n","Epoch 56/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.3229\n","Epoch 56: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 165ms/step - loss: 0.0747 - accuracy: 0.3229 - val_loss: 0.0956 - val_accuracy: 0.2889\n","Epoch 57/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.3494\n","Epoch 57: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 130ms/step - loss: 0.0747 - accuracy: 0.3494 - val_loss: 0.1085 - val_accuracy: 0.2000\n","Epoch 58/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.3325\n","Epoch 58: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 156ms/step - loss: 0.0702 - accuracy: 0.3325 - val_loss: 0.0997 - val_accuracy: 0.2444\n","Epoch 59/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0700 - accuracy: 0.3490\n","Epoch 59: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 126ms/step - loss: 0.0667 - accuracy: 0.3880 - val_loss: 0.1049 - val_accuracy: 0.2222\n","Epoch 60/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0652 - accuracy: 0.4141\n","Epoch 60: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 146ms/step - loss: 0.0623 - accuracy: 0.4410 - val_loss: 0.1016 - val_accuracy: 0.2000\n","Epoch 61/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0609 - accuracy: 0.4361\n","Epoch 61: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 155ms/step - loss: 0.0609 - accuracy: 0.4361 - val_loss: 0.1017 - val_accuracy: 0.2667\n","Epoch 62/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0622 - accuracy: 0.4323\n","Epoch 62: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 132ms/step - loss: 0.0590 - accuracy: 0.4627 - val_loss: 0.1021 - val_accuracy: 0.3111\n","Epoch 63/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0557 - accuracy: 0.5104\n","Epoch 63: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 127ms/step - loss: 0.0527 - accuracy: 0.5422 - val_loss: 0.0973 - val_accuracy: 0.2667\n","Epoch 64/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0525 - accuracy: 0.5398\n","Epoch 64: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 127ms/step - loss: 0.0525 - accuracy: 0.5398 - val_loss: 0.1066 - val_accuracy: 0.2889\n","Epoch 65/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.6024\n","Epoch 65: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 156ms/step - loss: 0.0484 - accuracy: 0.6024 - val_loss: 0.0977 - val_accuracy: 0.2889\n","Epoch 66/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0492 - accuracy: 0.5639\n","Epoch 66: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 128ms/step - loss: 0.0492 - accuracy: 0.5639 - val_loss: 0.1065 - val_accuracy: 0.2667\n","Epoch 67/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0443 - accuracy: 0.6313\n","Epoch 67: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 126ms/step - loss: 0.0443 - accuracy: 0.6313 - val_loss: 0.1039 - val_accuracy: 0.3111\n","Epoch 68/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0473 - accuracy: 0.6048\n","Epoch 68: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 130ms/step - loss: 0.0473 - accuracy: 0.6048 - val_loss: 0.1066 - val_accuracy: 0.2667\n","Epoch 69/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0449 - accuracy: 0.6265\n","Epoch 69: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 124ms/step - loss: 0.0449 - accuracy: 0.6265 - val_loss: 0.1039 - val_accuracy: 0.2889\n","Epoch 70/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.6120\n","Epoch 70: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 124ms/step - loss: 0.0442 - accuracy: 0.6120 - val_loss: 0.1112 - val_accuracy: 0.3333\n","Epoch 71/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0496 - accuracy: 0.5759\n","Epoch 71: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 124ms/step - loss: 0.0496 - accuracy: 0.5759 - val_loss: 0.1121 - val_accuracy: 0.2667\n","Epoch 72/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0486 - accuracy: 0.5677\n","Epoch 72: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 123ms/step - loss: 0.0461 - accuracy: 0.5976 - val_loss: 0.1007 - val_accuracy: 0.3556\n","Epoch 73/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0462 - accuracy: 0.5783\n","Epoch 73: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 123ms/step - loss: 0.0462 - accuracy: 0.5783 - val_loss: 0.1154 - val_accuracy: 0.2889\n","Epoch 74/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0483 - accuracy: 0.5781\n","Epoch 74: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 124ms/step - loss: 0.0459 - accuracy: 0.6024 - val_loss: 0.1024 - val_accuracy: 0.3333\n","Epoch 75/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0532 - accuracy: 0.4940\n","Epoch 75: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 128ms/step - loss: 0.0532 - accuracy: 0.4940 - val_loss: 0.1096 - val_accuracy: 0.3111\n","Epoch 76/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0472 - accuracy: 0.6024\n","Epoch 76: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 125ms/step - loss: 0.0472 - accuracy: 0.6024 - val_loss: 0.1074 - val_accuracy: 0.2889\n","Epoch 77/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0466 - accuracy: 0.6000\n","Epoch 77: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 125ms/step - loss: 0.0466 - accuracy: 0.6000 - val_loss: 0.1048 - val_accuracy: 0.3333\n","Epoch 78/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0425 - accuracy: 0.6410\n","Epoch 78: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 124ms/step - loss: 0.0425 - accuracy: 0.6410 - val_loss: 0.1024 - val_accuracy: 0.3333\n","Epoch 79/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0427 - accuracy: 0.6627\n","Epoch 79: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 157ms/step - loss: 0.0427 - accuracy: 0.6627 - val_loss: 0.1113 - val_accuracy: 0.2889\n","Epoch 80/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0484 - accuracy: 0.5651\n","Epoch 80: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 126ms/step - loss: 0.0462 - accuracy: 0.5904 - val_loss: 0.1160 - val_accuracy: 0.3556\n","Epoch 81/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.4120\n","Epoch 81: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 159ms/step - loss: 0.0698 - accuracy: 0.4120 - val_loss: 0.1122 - val_accuracy: 0.1556\n","Epoch 82/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0641 - accuracy: 0.4434\n","Epoch 82: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 128ms/step - loss: 0.0641 - accuracy: 0.4434 - val_loss: 0.1172 - val_accuracy: 0.2667\n","Epoch 83/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.5325\n","Epoch 83: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 125ms/step - loss: 0.0528 - accuracy: 0.5325 - val_loss: 0.1056 - val_accuracy: 0.2667\n","Epoch 84/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.5542\n","Epoch 84: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 172ms/step - loss: 0.0484 - accuracy: 0.5542 - val_loss: 0.1204 - val_accuracy: 0.2889\n","Epoch 85/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.5952\n","Epoch 85: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 156ms/step - loss: 0.0467 - accuracy: 0.5952 - val_loss: 0.1043 - val_accuracy: 0.3333\n","Epoch 86/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0507 - accuracy: 0.6000\n","Epoch 86: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 155ms/step - loss: 0.0507 - accuracy: 0.6000 - val_loss: 0.1082 - val_accuracy: 0.3778\n","Epoch 87/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.5759\n","Epoch 87: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 130ms/step - loss: 0.0467 - accuracy: 0.5759 - val_loss: 0.1086 - val_accuracy: 0.3556\n","Epoch 88/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0436 - accuracy: 0.6250\n","Epoch 88: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 125ms/step - loss: 0.0414 - accuracy: 0.6458 - val_loss: 0.1035 - val_accuracy: 0.4000\n","Epoch 89/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.5711\n","Epoch 89: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 125ms/step - loss: 0.0459 - accuracy: 0.5711 - val_loss: 0.1057 - val_accuracy: 0.3778\n","Epoch 90/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.5904\n","Epoch 90: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 127ms/step - loss: 0.0442 - accuracy: 0.5904 - val_loss: 0.1041 - val_accuracy: 0.4222\n","Epoch 91/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0417 - accuracy: 0.6675\n","Epoch 91: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 155ms/step - loss: 0.0417 - accuracy: 0.6675 - val_loss: 0.1045 - val_accuracy: 0.4222\n","Epoch 92/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.6458\n","Epoch 92: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 125ms/step - loss: 0.0407 - accuracy: 0.6458 - val_loss: 0.0998 - val_accuracy: 0.4000\n","Epoch 93/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.6843\n","Epoch 93: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 124ms/step - loss: 0.0361 - accuracy: 0.6843 - val_loss: 0.1012 - val_accuracy: 0.4444\n","Epoch 94/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.7181\n","Epoch 94: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 128ms/step - loss: 0.0329 - accuracy: 0.7181 - val_loss: 0.1060 - val_accuracy: 0.4000\n","Epoch 95/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.7373\n","Epoch 95: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 120ms/step - loss: 0.0318 - accuracy: 0.7373 - val_loss: 0.1156 - val_accuracy: 0.4222\n","Epoch 96/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.8000\n","Epoch 96: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 125ms/step - loss: 0.0271 - accuracy: 0.8000 - val_loss: 0.1066 - val_accuracy: 0.3778\n","Epoch 97/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.8241\n","Epoch 97: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 126ms/step - loss: 0.0248 - accuracy: 0.8241 - val_loss: 0.1024 - val_accuracy: 0.4667\n","Epoch 98/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.8217\n","Epoch 98: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 123ms/step - loss: 0.0231 - accuracy: 0.8217 - val_loss: 0.1121 - val_accuracy: 0.4000\n","Epoch 99/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.8410\n","Epoch 99: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 159ms/step - loss: 0.0216 - accuracy: 0.8410 - val_loss: 0.1133 - val_accuracy: 0.4667\n","Epoch 100/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.8530\n","Epoch 100: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 124ms/step - loss: 0.0200 - accuracy: 0.8530 - val_loss: 0.1171 - val_accuracy: 0.4667\n","Epoch 101/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.8651\n","Epoch 101: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 155ms/step - loss: 0.0184 - accuracy: 0.8651 - val_loss: 0.1155 - val_accuracy: 0.4222\n","Epoch 102/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.8554\n","Epoch 102: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 155ms/step - loss: 0.0199 - accuracy: 0.8554 - val_loss: 0.1299 - val_accuracy: 0.4667\n","Epoch 103/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0199 - accuracy: 0.8464\n","Epoch 103: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 127ms/step - loss: 0.0186 - accuracy: 0.8578 - val_loss: 0.1275 - val_accuracy: 0.4444\n","Epoch 104/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.8217\n","Epoch 104: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 130ms/step - loss: 0.0223 - accuracy: 0.8217 - val_loss: 0.1215 - val_accuracy: 0.4889\n","Epoch 105/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.8675\n","Epoch 105: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 123ms/step - loss: 0.0185 - accuracy: 0.8675 - val_loss: 0.1260 - val_accuracy: 0.4000\n","Epoch 106/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.8072\n","Epoch 106: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 131ms/step - loss: 0.0242 - accuracy: 0.8072 - val_loss: 0.1293 - val_accuracy: 0.4222\n","Epoch 107/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.7422\n","Epoch 107: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 154ms/step - loss: 0.0294 - accuracy: 0.7422 - val_loss: 0.1261 - val_accuracy: 0.3111\n","Epoch 108/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.7349\n","Epoch 108: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 135ms/step - loss: 0.0335 - accuracy: 0.7349 - val_loss: 0.1333 - val_accuracy: 0.3778\n","Epoch 109/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 0.7831\n","Epoch 109: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 135ms/step - loss: 0.0292 - accuracy: 0.7831 - val_loss: 0.1242 - val_accuracy: 0.3111\n","Epoch 110/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0325 - accuracy: 0.7370\n","Epoch 110: val_loss did not improve from 0.09535\n","4/4 [==============================] - 0s 121ms/step - loss: 0.0304 - accuracy: 0.7542 - val_loss: 0.1487 - val_accuracy: 0.3778\n","Epoch 111/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0471 - accuracy: 0.6265\n","Epoch 111: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 123ms/step - loss: 0.0471 - accuracy: 0.6265 - val_loss: 0.1279 - val_accuracy: 0.3333\n","Epoch 112/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0489 - accuracy: 0.6410\n","Epoch 112: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 123ms/step - loss: 0.0489 - accuracy: 0.6410 - val_loss: 0.1183 - val_accuracy: 0.3556\n","Epoch 113/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.6530\n","Epoch 113: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 124ms/step - loss: 0.0403 - accuracy: 0.6530 - val_loss: 0.1227 - val_accuracy: 0.2667\n","Epoch 114/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0498 - accuracy: 0.6193\n","Epoch 114: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 158ms/step - loss: 0.0498 - accuracy: 0.6193 - val_loss: 0.1185 - val_accuracy: 0.3778\n","Epoch 115/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.5904\n","Epoch 115: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 158ms/step - loss: 0.0478 - accuracy: 0.5904 - val_loss: 0.1204 - val_accuracy: 0.3556\n","Epoch 116/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0458 - accuracy: 0.6042\n","Epoch 116: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 128ms/step - loss: 0.0431 - accuracy: 0.6289 - val_loss: 0.1159 - val_accuracy: 0.3111\n","Epoch 117/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 0.6988\n","Epoch 117: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 154ms/step - loss: 0.0359 - accuracy: 0.6988 - val_loss: 0.1146 - val_accuracy: 0.3333\n","Epoch 118/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 0.7687\n","Epoch 118: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 128ms/step - loss: 0.0300 - accuracy: 0.7687 - val_loss: 0.1040 - val_accuracy: 0.3556\n","Epoch 119/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0248 - accuracy: 0.8073\n","Epoch 119: val_loss did not improve from 0.09535\n","4/4 [==============================] - 0s 122ms/step - loss: 0.0233 - accuracy: 0.8217 - val_loss: 0.1172 - val_accuracy: 0.4444\n","Epoch 120/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.7928\n","Epoch 120: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 157ms/step - loss: 0.0273 - accuracy: 0.7928 - val_loss: 0.1131 - val_accuracy: 0.4444\n","Epoch 121/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0306 - accuracy: 0.7708\n","Epoch 121: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 129ms/step - loss: 0.0285 - accuracy: 0.7880 - val_loss: 0.1140 - val_accuracy: 0.4000\n","Epoch 122/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.7807\n","Epoch 122: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 132ms/step - loss: 0.0275 - accuracy: 0.7807 - val_loss: 0.1211 - val_accuracy: 0.3778\n","Epoch 123/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0259 - accuracy: 0.8203\n","Epoch 123: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 124ms/step - loss: 0.0243 - accuracy: 0.8337 - val_loss: 0.1099 - val_accuracy: 0.4444\n","Epoch 124/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.8578\n","Epoch 124: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 123ms/step - loss: 0.0199 - accuracy: 0.8578 - val_loss: 0.1098 - val_accuracy: 0.4889\n","Epoch 125/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.8506\n","Epoch 125: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 156ms/step - loss: 0.0181 - accuracy: 0.8506 - val_loss: 0.1119 - val_accuracy: 0.4444\n","Epoch 126/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0175 - accuracy: 0.8646\n","Epoch 126: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 127ms/step - loss: 0.0163 - accuracy: 0.8747 - val_loss: 0.1256 - val_accuracy: 0.4667\n","Epoch 127/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0157 - accuracy: 0.8932\n","Epoch 127: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 129ms/step - loss: 0.0146 - accuracy: 0.9012 - val_loss: 0.1367 - val_accuracy: 0.4444\n","Epoch 128/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.8337\n","Epoch 128: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 129ms/step - loss: 0.0211 - accuracy: 0.8337 - val_loss: 0.1339 - val_accuracy: 0.4889\n","Epoch 129/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0201 - accuracy: 0.8568\n","Epoch 129: val_loss did not improve from 0.09535\n","4/4 [==============================] - 0s 123ms/step - loss: 0.0188 - accuracy: 0.8675 - val_loss: 0.1255 - val_accuracy: 0.4444\n","Epoch 130/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.8602\n","Epoch 130: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 154ms/step - loss: 0.0191 - accuracy: 0.8602 - val_loss: 0.1376 - val_accuracy: 0.4667\n","Epoch 131/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.8313\n","Epoch 131: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 127ms/step - loss: 0.0210 - accuracy: 0.8313 - val_loss: 0.1341 - val_accuracy: 0.4222\n","Epoch 132/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.8096\n","Epoch 132: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 162ms/step - loss: 0.0243 - accuracy: 0.8096 - val_loss: 0.1206 - val_accuracy: 0.4444\n","Epoch 133/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.8217\n","Epoch 133: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 158ms/step - loss: 0.0233 - accuracy: 0.8217 - val_loss: 0.1330 - val_accuracy: 0.4000\n","Epoch 134/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.7807\n","Epoch 134: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 157ms/step - loss: 0.0273 - accuracy: 0.7807 - val_loss: 0.1307 - val_accuracy: 0.3556\n","Epoch 135/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0206 - accuracy: 0.8438\n","Epoch 135: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 127ms/step - loss: 0.0199 - accuracy: 0.8506 - val_loss: 0.1104 - val_accuracy: 0.4667\n","Epoch 136/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.8964\n","Epoch 136: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 156ms/step - loss: 0.0166 - accuracy: 0.8964 - val_loss: 0.1223 - val_accuracy: 0.4667\n","Epoch 137/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9036\n","Epoch 137: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 133ms/step - loss: 0.0139 - accuracy: 0.9036 - val_loss: 0.1357 - val_accuracy: 0.4222\n","Epoch 138/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9157\n","Epoch 138: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 155ms/step - loss: 0.0135 - accuracy: 0.9157 - val_loss: 0.1158 - val_accuracy: 0.4889\n","Epoch 139/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.9398\n","Epoch 139: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 124ms/step - loss: 0.0103 - accuracy: 0.9398 - val_loss: 0.1243 - val_accuracy: 0.4667\n","Epoch 140/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0102 - accuracy: 0.9590\n","Epoch 140: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 157ms/step - loss: 0.0102 - accuracy: 0.9590 - val_loss: 0.1223 - val_accuracy: 0.4889\n","Epoch 141/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0082 - accuracy: 0.9531\n","Epoch 141: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 131ms/step - loss: 0.0077 - accuracy: 0.9566 - val_loss: 0.1240 - val_accuracy: 0.4667\n","Epoch 142/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0069 - accuracy: 0.9663\n","Epoch 142: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 129ms/step - loss: 0.0069 - accuracy: 0.9663 - val_loss: 0.1310 - val_accuracy: 0.4444\n","Epoch 143/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0056 - accuracy: 0.9783\n","Epoch 143: val_loss did not improve from 0.09535\n","4/4 [==============================] - 0s 123ms/step - loss: 0.0056 - accuracy: 0.9783 - val_loss: 0.1434 - val_accuracy: 0.4444\n","Epoch 144/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0058 - accuracy: 0.9663\n","Epoch 144: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 127ms/step - loss: 0.0058 - accuracy: 0.9663 - val_loss: 0.1363 - val_accuracy: 0.5111\n","Epoch 145/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0050 - accuracy: 0.9714\n","Epoch 145: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 129ms/step - loss: 0.0047 - accuracy: 0.9735 - val_loss: 0.1446 - val_accuracy: 0.4889\n","Epoch 146/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0051 - accuracy: 0.9844\n","Epoch 146: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 129ms/step - loss: 0.0048 - accuracy: 0.9855 - val_loss: 0.1478 - val_accuracy: 0.4444\n","Epoch 147/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0047 - accuracy: 0.9783\n","Epoch 147: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 129ms/step - loss: 0.0047 - accuracy: 0.9783 - val_loss: 0.1562 - val_accuracy: 0.4667\n","Epoch 148/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0072 - accuracy: 0.9398\n","Epoch 148: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 155ms/step - loss: 0.0072 - accuracy: 0.9398 - val_loss: 0.1494 - val_accuracy: 0.4889\n","Epoch 149/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0057 - accuracy: 0.9639\n","Epoch 149: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 125ms/step - loss: 0.0057 - accuracy: 0.9639 - val_loss: 0.1560 - val_accuracy: 0.4889\n","Epoch 150/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0068 - accuracy: 0.9614\n","Epoch 150: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 133ms/step - loss: 0.0068 - accuracy: 0.9614 - val_loss: 0.1421 - val_accuracy: 0.4667\n","Epoch 151/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0126 - accuracy: 0.9167\n","Epoch 151: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 126ms/step - loss: 0.0117 - accuracy: 0.9229 - val_loss: 0.1436 - val_accuracy: 0.4667\n","Epoch 152/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0134 - accuracy: 0.9193\n","Epoch 152: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 126ms/step - loss: 0.0126 - accuracy: 0.9229 - val_loss: 0.1789 - val_accuracy: 0.3778\n","Epoch 153/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0181 - accuracy: 0.8828\n","Epoch 153: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 131ms/step - loss: 0.0175 - accuracy: 0.8819 - val_loss: 0.1447 - val_accuracy: 0.4000\n","Epoch 154/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.8530\n","Epoch 154: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 125ms/step - loss: 0.0234 - accuracy: 0.8530 - val_loss: 0.1618 - val_accuracy: 0.4667\n","Epoch 155/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.8313\n","Epoch 155: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 160ms/step - loss: 0.0212 - accuracy: 0.8313 - val_loss: 0.1284 - val_accuracy: 0.4222\n","Epoch 156/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.7687\n","Epoch 156: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 128ms/step - loss: 0.0317 - accuracy: 0.7687 - val_loss: 0.1288 - val_accuracy: 0.4222\n","Epoch 157/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 0.7759\n","Epoch 157: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 130ms/step - loss: 0.0327 - accuracy: 0.7759 - val_loss: 0.1607 - val_accuracy: 0.3333\n","Epoch 158/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0395 - accuracy: 0.7344\n","Epoch 158: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 123ms/step - loss: 0.0370 - accuracy: 0.7518 - val_loss: 0.1235 - val_accuracy: 0.3778\n","Epoch 159/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0339 - accuracy: 0.7084\n","Epoch 159: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 157ms/step - loss: 0.0339 - accuracy: 0.7084 - val_loss: 0.1128 - val_accuracy: 0.4667\n","Epoch 160/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0253 - accuracy: 0.8125\n","Epoch 160: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 138ms/step - loss: 0.0244 - accuracy: 0.8193 - val_loss: 0.1160 - val_accuracy: 0.4667\n","Epoch 161/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0253 - accuracy: 0.8177\n","Epoch 161: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 127ms/step - loss: 0.0254 - accuracy: 0.8169 - val_loss: 0.1147 - val_accuracy: 0.4667\n","Epoch 162/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.8193\n","Epoch 162: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 128ms/step - loss: 0.0255 - accuracy: 0.8193 - val_loss: 0.1312 - val_accuracy: 0.4222\n","Epoch 163/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0328 - accuracy: 0.7448\n","Epoch 163: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 136ms/step - loss: 0.0306 - accuracy: 0.7639 - val_loss: 0.1326 - val_accuracy: 0.3111\n","Epoch 164/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0293 - accuracy: 0.7630\n","Epoch 164: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 125ms/step - loss: 0.0274 - accuracy: 0.7807 - val_loss: 0.1278 - val_accuracy: 0.4667\n","Epoch 165/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.7590\n","Epoch 165: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 125ms/step - loss: 0.0331 - accuracy: 0.7590 - val_loss: 0.1360 - val_accuracy: 0.3333\n","Epoch 166/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0427 - accuracy: 0.6843\n","Epoch 166: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 131ms/step - loss: 0.0427 - accuracy: 0.6843 - val_loss: 0.1242 - val_accuracy: 0.3778\n","Epoch 167/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0338 - accuracy: 0.7301\n","Epoch 167: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 126ms/step - loss: 0.0338 - accuracy: 0.7301 - val_loss: 0.1227 - val_accuracy: 0.3333\n","Epoch 168/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 0.7084\n","Epoch 168: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 132ms/step - loss: 0.0350 - accuracy: 0.7084 - val_loss: 0.1131 - val_accuracy: 0.4000\n","Epoch 169/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0265 - accuracy: 0.7943\n","Epoch 169: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 134ms/step - loss: 0.0253 - accuracy: 0.8048 - val_loss: 0.1221 - val_accuracy: 0.3778\n","Epoch 170/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.8530\n","Epoch 170: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 123ms/step - loss: 0.0201 - accuracy: 0.8530 - val_loss: 0.1182 - val_accuracy: 0.4000\n","Epoch 171/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9133\n","Epoch 171: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 164ms/step - loss: 0.0144 - accuracy: 0.9133 - val_loss: 0.1215 - val_accuracy: 0.5111\n","Epoch 172/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9253\n","Epoch 172: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 127ms/step - loss: 0.0122 - accuracy: 0.9253 - val_loss: 0.1257 - val_accuracy: 0.4889\n","Epoch 173/300\n","3/4 [=====================>........] - ETA: 0s - loss: 0.0084 - accuracy: 0.9714\n","Epoch 173: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 131ms/step - loss: 0.0079 - accuracy: 0.9735 - val_loss: 0.1246 - val_accuracy: 0.4444\n","Epoch 174/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 0.9687\n","Epoch 174: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 129ms/step - loss: 0.0066 - accuracy: 0.9687 - val_loss: 0.1279 - val_accuracy: 0.5556\n","Epoch 175/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0054 - accuracy: 0.9831\n","Epoch 175: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 163ms/step - loss: 0.0054 - accuracy: 0.9831 - val_loss: 0.1310 - val_accuracy: 0.5556\n","Epoch 176/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0040 - accuracy: 0.9928\n","Epoch 176: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 159ms/step - loss: 0.0040 - accuracy: 0.9928 - val_loss: 0.1329 - val_accuracy: 0.5778\n","Epoch 177/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0033 - accuracy: 0.9904\n","Epoch 177: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 136ms/step - loss: 0.0033 - accuracy: 0.9904 - val_loss: 0.1413 - val_accuracy: 0.5333\n","Epoch 178/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 0.9928\n","Epoch 178: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 159ms/step - loss: 0.0025 - accuracy: 0.9928 - val_loss: 0.1401 - val_accuracy: 0.5778\n","Epoch 179/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 0.9976\n","Epoch 179: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 159ms/step - loss: 0.0019 - accuracy: 0.9976 - val_loss: 0.1425 - val_accuracy: 0.6222\n","Epoch 180/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 0.9976\n","Epoch 180: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 159ms/step - loss: 0.0018 - accuracy: 0.9976 - val_loss: 0.1481 - val_accuracy: 0.6000\n","Epoch 181/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9976\n","Epoch 181: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 127ms/step - loss: 0.0015 - accuracy: 0.9976 - val_loss: 0.1527 - val_accuracy: 0.6000\n","Epoch 182/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000    \n","Epoch 182: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 162ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1558 - val_accuracy: 0.5778\n","Epoch 183/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000    \n","Epoch 183: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 124ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1581 - val_accuracy: 0.5778\n","Epoch 184/300\n","4/4 [==============================] - ETA: 0s - loss: 9.4153e-04 - accuracy: 1.0000\n","Epoch 184: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 160ms/step - loss: 9.4153e-04 - accuracy: 1.0000 - val_loss: 0.1596 - val_accuracy: 0.6000\n","Epoch 185/300\n","3/4 [=====================>........] - ETA: 0s - loss: 9.6274e-04 - accuracy: 1.0000\n","Epoch 185: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 125ms/step - loss: 8.9467e-04 - accuracy: 1.0000 - val_loss: 0.1620 - val_accuracy: 0.5778\n","Epoch 186/300\n","3/4 [=====================>........] - ETA: 0s - loss: 8.4535e-04 - accuracy: 1.0000\n","Epoch 186: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 131ms/step - loss: 7.8500e-04 - accuracy: 1.0000 - val_loss: 0.1637 - val_accuracy: 0.5778\n","Epoch 187/300\n","4/4 [==============================] - ETA: 0s - loss: 6.8196e-04 - accuracy: 1.0000\n","Epoch 187: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 126ms/step - loss: 6.8196e-04 - accuracy: 1.0000 - val_loss: 0.1622 - val_accuracy: 0.5778\n","Epoch 188/300\n","3/4 [=====================>........] - ETA: 0s - loss: 6.1800e-04 - accuracy: 1.0000\n","Epoch 188: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 124ms/step - loss: 5.7450e-04 - accuracy: 1.0000 - val_loss: 0.1627 - val_accuracy: 0.5778\n","Epoch 189/300\n","4/4 [==============================] - ETA: 0s - loss: 5.3182e-04 - accuracy: 1.0000\n","Epoch 189: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 125ms/step - loss: 5.3182e-04 - accuracy: 1.0000 - val_loss: 0.1658 - val_accuracy: 0.5778\n","Epoch 190/300\n","4/4 [==============================] - ETA: 0s - loss: 4.9477e-04 - accuracy: 1.0000\n","Epoch 190: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 124ms/step - loss: 4.9477e-04 - accuracy: 1.0000 - val_loss: 0.1678 - val_accuracy: 0.5778\n","Epoch 191/300\n","4/4 [==============================] - ETA: 0s - loss: 4.8392e-04 - accuracy: 1.0000\n","Epoch 191: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 126ms/step - loss: 4.8392e-04 - accuracy: 1.0000 - val_loss: 0.1686 - val_accuracy: 0.5778\n","Epoch 192/300\n","3/4 [=====================>........] - ETA: 0s - loss: 4.5288e-04 - accuracy: 1.0000\n","Epoch 192: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 129ms/step - loss: 4.2099e-04 - accuracy: 1.0000 - val_loss: 0.1699 - val_accuracy: 0.5778\n","Epoch 193/300\n","3/4 [=====================>........] - ETA: 0s - loss: 4.4263e-04 - accuracy: 1.0000\n","Epoch 193: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 129ms/step - loss: 4.1153e-04 - accuracy: 1.0000 - val_loss: 0.1713 - val_accuracy: 0.5778\n","Epoch 194/300\n","3/4 [=====================>........] - ETA: 0s - loss: 4.0651e-04 - accuracy: 1.0000\n","Epoch 194: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 125ms/step - loss: 3.7801e-04 - accuracy: 1.0000 - val_loss: 0.1728 - val_accuracy: 0.5778\n","Epoch 195/300\n","4/4 [==============================] - ETA: 0s - loss: 3.3698e-04 - accuracy: 1.0000\n","Epoch 195: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 159ms/step - loss: 3.3698e-04 - accuracy: 1.0000 - val_loss: 0.1731 - val_accuracy: 0.5778\n","Epoch 196/300\n","3/4 [=====================>........] - ETA: 0s - loss: 3.4679e-04 - accuracy: 1.0000\n","Epoch 196: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 135ms/step - loss: 3.2252e-04 - accuracy: 1.0000 - val_loss: 0.1739 - val_accuracy: 0.5778\n","Epoch 197/300\n","4/4 [==============================] - ETA: 0s - loss: 3.1242e-04 - accuracy: 1.0000\n","Epoch 197: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 128ms/step - loss: 3.1242e-04 - accuracy: 1.0000 - val_loss: 0.1758 - val_accuracy: 0.5778\n","Epoch 198/300\n","4/4 [==============================] - ETA: 0s - loss: 2.7861e-04 - accuracy: 1.0000\n","Epoch 198: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 164ms/step - loss: 2.7861e-04 - accuracy: 1.0000 - val_loss: 0.1764 - val_accuracy: 0.5778\n","Epoch 199/300\n","3/4 [=====================>........] - ETA: 0s - loss: 2.9411e-04 - accuracy: 1.0000\n","Epoch 199: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 126ms/step - loss: 2.7369e-04 - accuracy: 1.0000 - val_loss: 0.1767 - val_accuracy: 0.5778\n","Epoch 200/300\n","4/4 [==============================] - ETA: 0s - loss: 2.5962e-04 - accuracy: 1.0000\n","Epoch 200: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 126ms/step - loss: 2.5962e-04 - accuracy: 1.0000 - val_loss: 0.1782 - val_accuracy: 0.5778\n","Epoch 201/300\n","3/4 [=====================>........] - ETA: 0s - loss: 2.6602e-04 - accuracy: 1.0000\n","Epoch 201: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 130ms/step - loss: 2.4750e-04 - accuracy: 1.0000 - val_loss: 0.1791 - val_accuracy: 0.5778\n","Epoch 202/300\n","4/4 [==============================] - ETA: 0s - loss: 2.3208e-04 - accuracy: 1.0000\n","Epoch 202: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 160ms/step - loss: 2.3208e-04 - accuracy: 1.0000 - val_loss: 0.1801 - val_accuracy: 0.5778\n","Epoch 203/300\n","4/4 [==============================] - ETA: 0s - loss: 2.2114e-04 - accuracy: 1.0000\n","Epoch 203: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 127ms/step - loss: 2.2114e-04 - accuracy: 1.0000 - val_loss: 0.1805 - val_accuracy: 0.5778\n","Epoch 204/300\n","4/4 [==============================] - ETA: 0s - loss: 2.1738e-04 - accuracy: 1.0000\n","Epoch 204: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 128ms/step - loss: 2.1738e-04 - accuracy: 1.0000 - val_loss: 0.1813 - val_accuracy: 0.5778\n","Epoch 205/300\n","4/4 [==============================] - ETA: 0s - loss: 2.1028e-04 - accuracy: 1.0000\n","Epoch 205: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 135ms/step - loss: 2.1028e-04 - accuracy: 1.0000 - val_loss: 0.1828 - val_accuracy: 0.5556\n","Epoch 206/300\n","4/4 [==============================] - ETA: 0s - loss: 2.0025e-04 - accuracy: 1.0000\n","Epoch 206: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 125ms/step - loss: 2.0025e-04 - accuracy: 1.0000 - val_loss: 0.1829 - val_accuracy: 0.5778\n","Epoch 207/300\n","4/4 [==============================] - ETA: 0s - loss: 1.9000e-04 - accuracy: 1.0000\n","Epoch 207: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 157ms/step - loss: 1.9000e-04 - accuracy: 1.0000 - val_loss: 0.1835 - val_accuracy: 0.5778\n","Epoch 208/300\n","4/4 [==============================] - ETA: 0s - loss: 1.8823e-04 - accuracy: 1.0000\n","Epoch 208: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 130ms/step - loss: 1.8823e-04 - accuracy: 1.0000 - val_loss: 0.1848 - val_accuracy: 0.5556\n","Epoch 209/300\n","4/4 [==============================] - ETA: 0s - loss: 1.7818e-04 - accuracy: 1.0000\n","Epoch 209: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 131ms/step - loss: 1.7818e-04 - accuracy: 1.0000 - val_loss: 0.1851 - val_accuracy: 0.5556\n","Epoch 210/300\n","4/4 [==============================] - ETA: 0s - loss: 1.7050e-04 - accuracy: 1.0000\n","Epoch 210: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 130ms/step - loss: 1.7050e-04 - accuracy: 1.0000 - val_loss: 0.1858 - val_accuracy: 0.5556\n","Epoch 211/300\n","4/4 [==============================] - ETA: 0s - loss: 1.6420e-04 - accuracy: 1.0000\n","Epoch 211: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 127ms/step - loss: 1.6420e-04 - accuracy: 1.0000 - val_loss: 0.1864 - val_accuracy: 0.5556\n","Epoch 212/300\n","4/4 [==============================] - ETA: 0s - loss: 1.5844e-04 - accuracy: 1.0000\n","Epoch 212: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 126ms/step - loss: 1.5844e-04 - accuracy: 1.0000 - val_loss: 0.1867 - val_accuracy: 0.5556\n","Epoch 213/300\n","4/4 [==============================] - ETA: 0s - loss: 1.5464e-04 - accuracy: 1.0000\n","Epoch 213: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 161ms/step - loss: 1.5464e-04 - accuracy: 1.0000 - val_loss: 0.1872 - val_accuracy: 0.5556\n","Epoch 214/300\n","4/4 [==============================] - ETA: 0s - loss: 1.5022e-04 - accuracy: 1.0000\n","Epoch 214: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 135ms/step - loss: 1.5022e-04 - accuracy: 1.0000 - val_loss: 0.1883 - val_accuracy: 0.5556\n","Epoch 215/300\n","4/4 [==============================] - ETA: 0s - loss: 1.4617e-04 - accuracy: 1.0000\n","Epoch 215: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 128ms/step - loss: 1.4617e-04 - accuracy: 1.0000 - val_loss: 0.1891 - val_accuracy: 0.5556\n","Epoch 216/300\n","3/4 [=====================>........] - ETA: 0s - loss: 1.5258e-04 - accuracy: 1.0000\n","Epoch 216: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 127ms/step - loss: 1.4205e-04 - accuracy: 1.0000 - val_loss: 0.1890 - val_accuracy: 0.5556\n","Epoch 217/300\n","4/4 [==============================] - ETA: 0s - loss: 1.3602e-04 - accuracy: 1.0000\n","Epoch 217: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 166ms/step - loss: 1.3602e-04 - accuracy: 1.0000 - val_loss: 0.1895 - val_accuracy: 0.5556\n","Epoch 218/300\n","4/4 [==============================] - ETA: 0s - loss: 1.3372e-04 - accuracy: 1.0000\n","Epoch 218: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 125ms/step - loss: 1.3372e-04 - accuracy: 1.0000 - val_loss: 0.1903 - val_accuracy: 0.5556\n","Epoch 219/300\n","4/4 [==============================] - ETA: 0s - loss: 1.2861e-04 - accuracy: 1.0000\n","Epoch 219: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 131ms/step - loss: 1.2861e-04 - accuracy: 1.0000 - val_loss: 0.1911 - val_accuracy: 0.5556\n","Epoch 220/300\n","3/4 [=====================>........] - ETA: 0s - loss: 1.3553e-04 - accuracy: 1.0000\n","Epoch 220: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 121ms/step - loss: 1.2619e-04 - accuracy: 1.0000 - val_loss: 0.1917 - val_accuracy: 0.5556\n","Epoch 221/300\n","3/4 [=====================>........] - ETA: 0s - loss: 1.3250e-04 - accuracy: 1.0000\n","Epoch 221: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 128ms/step - loss: 1.2340e-04 - accuracy: 1.0000 - val_loss: 0.1919 - val_accuracy: 0.5556\n","Epoch 222/300\n","4/4 [==============================] - ETA: 0s - loss: 1.1849e-04 - accuracy: 1.0000\n","Epoch 222: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 161ms/step - loss: 1.1849e-04 - accuracy: 1.0000 - val_loss: 0.1924 - val_accuracy: 0.5556\n","Epoch 223/300\n","4/4 [==============================] - ETA: 0s - loss: 1.1657e-04 - accuracy: 1.0000\n","Epoch 223: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 165ms/step - loss: 1.1657e-04 - accuracy: 1.0000 - val_loss: 0.1928 - val_accuracy: 0.5556\n","Epoch 224/300\n","4/4 [==============================] - ETA: 0s - loss: 1.1371e-04 - accuracy: 1.0000\n","Epoch 224: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 131ms/step - loss: 1.1371e-04 - accuracy: 1.0000 - val_loss: 0.1933 - val_accuracy: 0.5556\n","Epoch 225/300\n","4/4 [==============================] - ETA: 0s - loss: 1.1103e-04 - accuracy: 1.0000\n","Epoch 225: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 123ms/step - loss: 1.1103e-04 - accuracy: 1.0000 - val_loss: 0.1939 - val_accuracy: 0.5556\n","Epoch 226/300\n","4/4 [==============================] - ETA: 0s - loss: 1.0926e-04 - accuracy: 1.0000\n","Epoch 226: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 135ms/step - loss: 1.0926e-04 - accuracy: 1.0000 - val_loss: 0.1946 - val_accuracy: 0.5556\n","Epoch 227/300\n","3/4 [=====================>........] - ETA: 0s - loss: 1.1352e-04 - accuracy: 1.0000\n","Epoch 227: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 129ms/step - loss: 1.0571e-04 - accuracy: 1.0000 - val_loss: 0.1950 - val_accuracy: 0.5556\n","Epoch 228/300\n","4/4 [==============================] - ETA: 0s - loss: 1.0218e-04 - accuracy: 1.0000\n","Epoch 228: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 169ms/step - loss: 1.0218e-04 - accuracy: 1.0000 - val_loss: 0.1952 - val_accuracy: 0.5556\n","Epoch 229/300\n","4/4 [==============================] - ETA: 0s - loss: 1.0026e-04 - accuracy: 1.0000\n","Epoch 229: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 165ms/step - loss: 1.0026e-04 - accuracy: 1.0000 - val_loss: 0.1955 - val_accuracy: 0.5556\n","Epoch 230/300\n","4/4 [==============================] - ETA: 0s - loss: 9.9330e-05 - accuracy: 1.0000\n","Epoch 230: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 133ms/step - loss: 9.9330e-05 - accuracy: 1.0000 - val_loss: 0.1959 - val_accuracy: 0.5556\n","Epoch 231/300\n","4/4 [==============================] - ETA: 0s - loss: 9.6490e-05 - accuracy: 1.0000\n","Epoch 231: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 128ms/step - loss: 9.6490e-05 - accuracy: 1.0000 - val_loss: 0.1964 - val_accuracy: 0.5556\n","Epoch 232/300\n","4/4 [==============================] - ETA: 0s - loss: 9.5347e-05 - accuracy: 1.0000\n","Epoch 232: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 131ms/step - loss: 9.5347e-05 - accuracy: 1.0000 - val_loss: 0.1973 - val_accuracy: 0.5556\n","Epoch 233/300\n","4/4 [==============================] - ETA: 0s - loss: 9.2674e-05 - accuracy: 1.0000\n","Epoch 233: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 163ms/step - loss: 9.2674e-05 - accuracy: 1.0000 - val_loss: 0.1976 - val_accuracy: 0.5556\n","Epoch 234/300\n","4/4 [==============================] - ETA: 0s - loss: 9.0356e-05 - accuracy: 1.0000\n","Epoch 234: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 158ms/step - loss: 9.0356e-05 - accuracy: 1.0000 - val_loss: 0.1978 - val_accuracy: 0.5556\n","Epoch 235/300\n","4/4 [==============================] - ETA: 0s - loss: 8.8828e-05 - accuracy: 1.0000\n","Epoch 235: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 133ms/step - loss: 8.8828e-05 - accuracy: 1.0000 - val_loss: 0.1980 - val_accuracy: 0.5556\n","Epoch 236/300\n","4/4 [==============================] - ETA: 0s - loss: 8.6606e-05 - accuracy: 1.0000\n","Epoch 236: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 156ms/step - loss: 8.6606e-05 - accuracy: 1.0000 - val_loss: 0.1986 - val_accuracy: 0.5556\n","Epoch 237/300\n","4/4 [==============================] - ETA: 0s - loss: 8.5078e-05 - accuracy: 1.0000\n","Epoch 237: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 128ms/step - loss: 8.5078e-05 - accuracy: 1.0000 - val_loss: 0.1991 - val_accuracy: 0.5556\n","Epoch 238/300\n","3/4 [=====================>........] - ETA: 0s - loss: 8.9531e-05 - accuracy: 1.0000\n","Epoch 238: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 127ms/step - loss: 8.3391e-05 - accuracy: 1.0000 - val_loss: 0.1994 - val_accuracy: 0.5556\n","Epoch 239/300\n","4/4 [==============================] - ETA: 0s - loss: 8.2325e-05 - accuracy: 1.0000\n","Epoch 239: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 131ms/step - loss: 8.2325e-05 - accuracy: 1.0000 - val_loss: 0.1996 - val_accuracy: 0.5556\n","Epoch 240/300\n","3/4 [=====================>........] - ETA: 0s - loss: 8.6007e-05 - accuracy: 1.0000\n","Epoch 240: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 128ms/step - loss: 8.0122e-05 - accuracy: 1.0000 - val_loss: 0.1999 - val_accuracy: 0.5556\n","Epoch 241/300\n","4/4 [==============================] - ETA: 0s - loss: 7.8046e-05 - accuracy: 1.0000\n","Epoch 241: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 158ms/step - loss: 7.8046e-05 - accuracy: 1.0000 - val_loss: 0.2004 - val_accuracy: 0.5556\n","Epoch 242/300\n","3/4 [=====================>........] - ETA: 0s - loss: 8.2451e-05 - accuracy: 1.0000\n","Epoch 242: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 134ms/step - loss: 7.6798e-05 - accuracy: 1.0000 - val_loss: 0.2009 - val_accuracy: 0.5556\n","Epoch 243/300\n","4/4 [==============================] - ETA: 0s - loss: 7.5839e-05 - accuracy: 1.0000\n","Epoch 243: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 127ms/step - loss: 7.5839e-05 - accuracy: 1.0000 - val_loss: 0.2012 - val_accuracy: 0.5556\n","Epoch 244/300\n","3/4 [=====================>........] - ETA: 0s - loss: 7.9558e-05 - accuracy: 1.0000\n","Epoch 244: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 132ms/step - loss: 7.4113e-05 - accuracy: 1.0000 - val_loss: 0.2016 - val_accuracy: 0.5556\n","Epoch 245/300\n","4/4 [==============================] - ETA: 0s - loss: 7.2660e-05 - accuracy: 1.0000\n","Epoch 245: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 128ms/step - loss: 7.2660e-05 - accuracy: 1.0000 - val_loss: 0.2019 - val_accuracy: 0.5556\n","Epoch 246/300\n","3/4 [=====================>........] - ETA: 0s - loss: 7.6594e-05 - accuracy: 1.0000\n","Epoch 246: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 131ms/step - loss: 7.1350e-05 - accuracy: 1.0000 - val_loss: 0.2022 - val_accuracy: 0.5556\n","Epoch 247/300\n","4/4 [==============================] - ETA: 0s - loss: 6.9863e-05 - accuracy: 1.0000\n","Epoch 247: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 125ms/step - loss: 6.9863e-05 - accuracy: 1.0000 - val_loss: 0.2025 - val_accuracy: 0.5556\n","Epoch 248/300\n","3/4 [=====================>........] - ETA: 0s - loss: 7.4202e-05 - accuracy: 1.0000\n","Epoch 248: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 123ms/step - loss: 6.9126e-05 - accuracy: 1.0000 - val_loss: 0.2028 - val_accuracy: 0.5556\n","Epoch 249/300\n","3/4 [=====================>........] - ETA: 0s - loss: 7.2643e-05 - accuracy: 1.0000\n","Epoch 249: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 123ms/step - loss: 6.7676e-05 - accuracy: 1.0000 - val_loss: 0.2032 - val_accuracy: 0.5556\n","Epoch 250/300\n","3/4 [=====================>........] - ETA: 0s - loss: 7.1413e-05 - accuracy: 1.0000\n","Epoch 250: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 128ms/step - loss: 6.6527e-05 - accuracy: 1.0000 - val_loss: 0.2035 - val_accuracy: 0.5556\n","Epoch 251/300\n","4/4 [==============================] - ETA: 0s - loss: 6.5146e-05 - accuracy: 1.0000\n","Epoch 251: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 161ms/step - loss: 6.5146e-05 - accuracy: 1.0000 - val_loss: 0.2039 - val_accuracy: 0.5556\n","Epoch 252/300\n","4/4 [==============================] - ETA: 0s - loss: 6.4043e-05 - accuracy: 1.0000\n","Epoch 252: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 125ms/step - loss: 6.4043e-05 - accuracy: 1.0000 - val_loss: 0.2043 - val_accuracy: 0.5556\n","Epoch 253/300\n","4/4 [==============================] - ETA: 0s - loss: 6.3485e-05 - accuracy: 1.0000\n","Epoch 253: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 134ms/step - loss: 6.3485e-05 - accuracy: 1.0000 - val_loss: 0.2044 - val_accuracy: 0.5556\n","Epoch 254/300\n","4/4 [==============================] - ETA: 0s - loss: 6.2026e-05 - accuracy: 1.0000\n","Epoch 254: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 178ms/step - loss: 6.2026e-05 - accuracy: 1.0000 - val_loss: 0.2046 - val_accuracy: 0.5556\n","Epoch 255/300\n","4/4 [==============================] - ETA: 0s - loss: 6.0975e-05 - accuracy: 1.0000\n","Epoch 255: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 126ms/step - loss: 6.0975e-05 - accuracy: 1.0000 - val_loss: 0.2050 - val_accuracy: 0.5556\n","Epoch 256/300\n","4/4 [==============================] - ETA: 0s - loss: 5.9966e-05 - accuracy: 1.0000\n","Epoch 256: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 163ms/step - loss: 5.9966e-05 - accuracy: 1.0000 - val_loss: 0.2054 - val_accuracy: 0.5556\n","Epoch 257/300\n","4/4 [==============================] - ETA: 0s - loss: 5.9224e-05 - accuracy: 1.0000\n","Epoch 257: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 124ms/step - loss: 5.9224e-05 - accuracy: 1.0000 - val_loss: 0.2058 - val_accuracy: 0.5556\n","Epoch 258/300\n","3/4 [=====================>........] - ETA: 0s - loss: 6.2807e-05 - accuracy: 1.0000\n","Epoch 258: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 134ms/step - loss: 5.8516e-05 - accuracy: 1.0000 - val_loss: 0.2059 - val_accuracy: 0.5556\n","Epoch 259/300\n","3/4 [=====================>........] - ETA: 0s - loss: 6.1438e-05 - accuracy: 1.0000\n","Epoch 259: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 127ms/step - loss: 5.7243e-05 - accuracy: 1.0000 - val_loss: 0.2063 - val_accuracy: 0.5556\n","Epoch 260/300\n","4/4 [==============================] - ETA: 0s - loss: 5.6356e-05 - accuracy: 1.0000\n","Epoch 260: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 141ms/step - loss: 5.6356e-05 - accuracy: 1.0000 - val_loss: 0.2066 - val_accuracy: 0.5556\n","Epoch 261/300\n","4/4 [==============================] - ETA: 0s - loss: 5.5554e-05 - accuracy: 1.0000\n","Epoch 261: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 124ms/step - loss: 5.5554e-05 - accuracy: 1.0000 - val_loss: 0.2071 - val_accuracy: 0.5556\n","Epoch 262/300\n","4/4 [==============================] - ETA: 0s - loss: 5.4470e-05 - accuracy: 1.0000\n","Epoch 262: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 127ms/step - loss: 5.4470e-05 - accuracy: 1.0000 - val_loss: 0.2073 - val_accuracy: 0.5556\n","Epoch 263/300\n","4/4 [==============================] - ETA: 0s - loss: 5.3443e-05 - accuracy: 1.0000\n","Epoch 263: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 161ms/step - loss: 5.3443e-05 - accuracy: 1.0000 - val_loss: 0.2075 - val_accuracy: 0.5556\n","Epoch 264/300\n","3/4 [=====================>........] - ETA: 0s - loss: 5.6813e-05 - accuracy: 1.0000\n","Epoch 264: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 132ms/step - loss: 5.2938e-05 - accuracy: 1.0000 - val_loss: 0.2078 - val_accuracy: 0.5556\n","Epoch 265/300\n","3/4 [=====================>........] - ETA: 0s - loss: 5.5895e-05 - accuracy: 1.0000\n","Epoch 265: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 135ms/step - loss: 5.2083e-05 - accuracy: 1.0000 - val_loss: 0.2081 - val_accuracy: 0.5556\n","Epoch 266/300\n","4/4 [==============================] - ETA: 0s - loss: 5.1369e-05 - accuracy: 1.0000\n","Epoch 266: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 128ms/step - loss: 5.1369e-05 - accuracy: 1.0000 - val_loss: 0.2085 - val_accuracy: 0.5556\n","Epoch 267/300\n","4/4 [==============================] - ETA: 0s - loss: 5.0638e-05 - accuracy: 1.0000\n","Epoch 267: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 129ms/step - loss: 5.0638e-05 - accuracy: 1.0000 - val_loss: 0.2089 - val_accuracy: 0.5556\n","Epoch 268/300\n","3/4 [=====================>........] - ETA: 0s - loss: 5.3983e-05 - accuracy: 1.0000\n","Epoch 268: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 124ms/step - loss: 5.0299e-05 - accuracy: 1.0000 - val_loss: 0.2090 - val_accuracy: 0.5556\n","Epoch 269/300\n","4/4 [==============================] - ETA: 0s - loss: 4.9145e-05 - accuracy: 1.0000\n","Epoch 269: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 163ms/step - loss: 4.9145e-05 - accuracy: 1.0000 - val_loss: 0.2092 - val_accuracy: 0.5556\n","Epoch 270/300\n","4/4 [==============================] - ETA: 0s - loss: 4.8886e-05 - accuracy: 1.0000\n","Epoch 270: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 131ms/step - loss: 4.8886e-05 - accuracy: 1.0000 - val_loss: 0.2097 - val_accuracy: 0.5556\n","Epoch 271/300\n","3/4 [=====================>........] - ETA: 0s - loss: 5.1350e-05 - accuracy: 1.0000\n","Epoch 271: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 135ms/step - loss: 4.7843e-05 - accuracy: 1.0000 - val_loss: 0.2100 - val_accuracy: 0.5556\n","Epoch 272/300\n","3/4 [=====================>........] - ETA: 0s - loss: 5.0703e-05 - accuracy: 1.0000\n","Epoch 272: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 129ms/step - loss: 4.7246e-05 - accuracy: 1.0000 - val_loss: 0.2102 - val_accuracy: 0.5556\n","Epoch 273/300\n","4/4 [==============================] - ETA: 0s - loss: 4.6525e-05 - accuracy: 1.0000\n","Epoch 273: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 130ms/step - loss: 4.6525e-05 - accuracy: 1.0000 - val_loss: 0.2105 - val_accuracy: 0.5556\n","Epoch 274/300\n","4/4 [==============================] - ETA: 0s - loss: 4.6080e-05 - accuracy: 1.0000\n","Epoch 274: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 130ms/step - loss: 4.6080e-05 - accuracy: 1.0000 - val_loss: 0.2107 - val_accuracy: 0.5556\n","Epoch 275/300\n","4/4 [==============================] - ETA: 0s - loss: 4.5550e-05 - accuracy: 1.0000\n","Epoch 275: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 125ms/step - loss: 4.5550e-05 - accuracy: 1.0000 - val_loss: 0.2111 - val_accuracy: 0.5556\n","Epoch 276/300\n","4/4 [==============================] - ETA: 0s - loss: 4.4677e-05 - accuracy: 1.0000\n","Epoch 276: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 132ms/step - loss: 4.4677e-05 - accuracy: 1.0000 - val_loss: 0.2113 - val_accuracy: 0.5556\n","Epoch 277/300\n","4/4 [==============================] - ETA: 0s - loss: 4.3985e-05 - accuracy: 1.0000\n","Epoch 277: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 135ms/step - loss: 4.3985e-05 - accuracy: 1.0000 - val_loss: 0.2116 - val_accuracy: 0.5556\n","Epoch 278/300\n","4/4 [==============================] - ETA: 0s - loss: 4.3282e-05 - accuracy: 1.0000\n","Epoch 278: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 158ms/step - loss: 4.3282e-05 - accuracy: 1.0000 - val_loss: 0.2118 - val_accuracy: 0.5556\n","Epoch 279/300\n","4/4 [==============================] - ETA: 0s - loss: 4.2711e-05 - accuracy: 1.0000\n","Epoch 279: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 157ms/step - loss: 4.2711e-05 - accuracy: 1.0000 - val_loss: 0.2120 - val_accuracy: 0.5556\n","Epoch 280/300\n","3/4 [=====================>........] - ETA: 0s - loss: 4.5486e-05 - accuracy: 1.0000\n","Epoch 280: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 125ms/step - loss: 4.2387e-05 - accuracy: 1.0000 - val_loss: 0.2122 - val_accuracy: 0.5556\n","Epoch 281/300\n","3/4 [=====================>........] - ETA: 0s - loss: 4.4941e-05 - accuracy: 1.0000\n","Epoch 281: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 133ms/step - loss: 4.1880e-05 - accuracy: 1.0000 - val_loss: 0.2124 - val_accuracy: 0.5556\n","Epoch 282/300\n","3/4 [=====================>........] - ETA: 0s - loss: 4.4311e-05 - accuracy: 1.0000\n","Epoch 282: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 127ms/step - loss: 4.1290e-05 - accuracy: 1.0000 - val_loss: 0.2127 - val_accuracy: 0.5556\n","Epoch 283/300\n","3/4 [=====================>........] - ETA: 0s - loss: 4.3717e-05 - accuracy: 1.0000\n","Epoch 283: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 129ms/step - loss: 4.0735e-05 - accuracy: 1.0000 - val_loss: 0.2131 - val_accuracy: 0.5556\n","Epoch 284/300\n","4/4 [==============================] - ETA: 0s - loss: 4.0280e-05 - accuracy: 1.0000\n","Epoch 284: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 129ms/step - loss: 4.0280e-05 - accuracy: 1.0000 - val_loss: 0.2133 - val_accuracy: 0.5556\n","Epoch 285/300\n","4/4 [==============================] - ETA: 0s - loss: 3.9642e-05 - accuracy: 1.0000\n","Epoch 285: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 163ms/step - loss: 3.9642e-05 - accuracy: 1.0000 - val_loss: 0.2134 - val_accuracy: 0.5556\n","Epoch 286/300\n","4/4 [==============================] - ETA: 0s - loss: 3.9436e-05 - accuracy: 1.0000\n","Epoch 286: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 127ms/step - loss: 3.9436e-05 - accuracy: 1.0000 - val_loss: 0.2135 - val_accuracy: 0.5556\n","Epoch 287/300\n","3/4 [=====================>........] - ETA: 0s - loss: 4.1563e-05 - accuracy: 1.0000\n","Epoch 287: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 129ms/step - loss: 3.8736e-05 - accuracy: 1.0000 - val_loss: 0.2138 - val_accuracy: 0.5556\n","Epoch 288/300\n","4/4 [==============================] - ETA: 0s - loss: 3.8581e-05 - accuracy: 1.0000\n","Epoch 288: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 131ms/step - loss: 3.8581e-05 - accuracy: 1.0000 - val_loss: 0.2142 - val_accuracy: 0.5556\n","Epoch 289/300\n","4/4 [==============================] - ETA: 0s - loss: 3.8054e-05 - accuracy: 1.0000\n","Epoch 289: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 131ms/step - loss: 3.8054e-05 - accuracy: 1.0000 - val_loss: 0.2144 - val_accuracy: 0.5556\n","Epoch 290/300\n","4/4 [==============================] - ETA: 0s - loss: 3.7439e-05 - accuracy: 1.0000\n","Epoch 290: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 130ms/step - loss: 3.7439e-05 - accuracy: 1.0000 - val_loss: 0.2146 - val_accuracy: 0.5556\n","Epoch 291/300\n","4/4 [==============================] - ETA: 0s - loss: 3.6848e-05 - accuracy: 1.0000\n","Epoch 291: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 162ms/step - loss: 3.6848e-05 - accuracy: 1.0000 - val_loss: 0.2148 - val_accuracy: 0.5556\n","Epoch 292/300\n","4/4 [==============================] - ETA: 0s - loss: 3.6772e-05 - accuracy: 1.0000\n","Epoch 292: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 128ms/step - loss: 3.6772e-05 - accuracy: 1.0000 - val_loss: 0.2150 - val_accuracy: 0.5556\n","Epoch 293/300\n","4/4 [==============================] - ETA: 0s - loss: 3.6089e-05 - accuracy: 1.0000\n","Epoch 293: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 124ms/step - loss: 3.6089e-05 - accuracy: 1.0000 - val_loss: 0.2151 - val_accuracy: 0.5556\n","Epoch 294/300\n","4/4 [==============================] - ETA: 0s - loss: 3.5586e-05 - accuracy: 1.0000\n","Epoch 294: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 130ms/step - loss: 3.5586e-05 - accuracy: 1.0000 - val_loss: 0.2154 - val_accuracy: 0.5556\n","Epoch 295/300\n","4/4 [==============================] - ETA: 0s - loss: 3.5477e-05 - accuracy: 1.0000\n","Epoch 295: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 138ms/step - loss: 3.5477e-05 - accuracy: 1.0000 - val_loss: 0.2157 - val_accuracy: 0.5556\n","Epoch 296/300\n","4/4 [==============================] - ETA: 0s - loss: 3.4904e-05 - accuracy: 1.0000\n","Epoch 296: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 127ms/step - loss: 3.4904e-05 - accuracy: 1.0000 - val_loss: 0.2159 - val_accuracy: 0.5556\n","Epoch 297/300\n","4/4 [==============================] - ETA: 0s - loss: 3.4313e-05 - accuracy: 1.0000\n","Epoch 297: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 158ms/step - loss: 3.4313e-05 - accuracy: 1.0000 - val_loss: 0.2160 - val_accuracy: 0.5556\n","Epoch 298/300\n","4/4 [==============================] - ETA: 0s - loss: 3.4133e-05 - accuracy: 1.0000\n","Epoch 298: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 129ms/step - loss: 3.4133e-05 - accuracy: 1.0000 - val_loss: 0.2163 - val_accuracy: 0.5556\n","Epoch 299/300\n","4/4 [==============================] - ETA: 0s - loss: 3.3640e-05 - accuracy: 1.0000\n","Epoch 299: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 126ms/step - loss: 3.3640e-05 - accuracy: 1.0000 - val_loss: 0.2166 - val_accuracy: 0.5556\n","Epoch 300/300\n","3/4 [=====================>........] - ETA: 0s - loss: 3.5557e-05 - accuracy: 1.0000\n","Epoch 300: val_loss did not improve from 0.09535\n","4/4 [==============================] - 1s 131ms/step - loss: 3.3142e-05 - accuracy: 1.0000 - val_loss: 0.2167 - val_accuracy: 0.5556\n"]}]},{"cell_type":"code","source":["plot_training(history)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":350},"id":"O3MWa-0VufpS","executionInfo":{"status":"ok","timestamp":1648077780664,"user_tz":0,"elapsed":480,"user":{"displayName":"Evern Joshua","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16916867645640256262"}},"outputId":"b4ae1b73-039d-4461-df5a-b79438ce3396"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1008x360 with 2 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["test_model(model,val_set)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bZNbN83Lt9RJ","executionInfo":{"status":"ok","timestamp":1648077793275,"user_tz":0,"elapsed":324,"user":{"displayName":"Evern Joshua","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16916867645640256262"}},"outputId":"d5d22863-6ddf-4aa2-8d59-d43f66f882fe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:Model was constructed with shape (None, 64, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, 64, 64), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape (None, None, None, None).\n","Model classification accurcy: 0.56\n","Classification Report\n","              precision    recall  f1-score   support\n","\n","           1       1.00      1.00      1.00         1\n","          10       0.00      0.00      0.00         1\n","          11       0.00      0.00      0.00         1\n","          12       1.00      1.00      1.00         1\n","          13       1.00      1.00      1.00         1\n","          14       0.00      0.00      0.00         1\n","          15       0.00      0.00      0.00         1\n","          16       0.00      0.00      0.00         1\n","          17       0.50      1.00      0.67         1\n","          18       0.00      0.00      0.00         1\n","          19       1.00      1.00      1.00         1\n","           2       0.00      0.00      0.00         1\n","          20       1.00      1.00      1.00         1\n","          21       0.00      0.00      0.00         1\n","          22       1.00      1.00      1.00         1\n","          23       0.00      0.00      0.00         1\n","          24       1.00      1.00      1.00         1\n","          25       0.50      1.00      0.67         1\n","          26       0.50      1.00      0.67         1\n","          27       0.33      1.00      0.50         1\n","          28       0.00      0.00      0.00         1\n","          29       0.00      0.00      0.00         1\n","           3       0.00      0.00      0.00         1\n","          30       1.00      1.00      1.00         1\n","          31       1.00      1.00      1.00         1\n","          32       1.00      1.00      1.00         1\n","          33       0.50      1.00      0.67         1\n","          34       0.00      0.00      0.00         1\n","          35       1.00      1.00      1.00         1\n","          36       0.00      0.00      0.00         1\n","          37       0.00      0.00      0.00         1\n","          38       1.00      1.00      1.00         1\n","          39       1.00      1.00      1.00         1\n","          40       0.00      0.00      0.00         1\n","          41       1.00      1.00      1.00         1\n","          42       1.00      1.00      1.00         1\n","          43       1.00      1.00      1.00         1\n","          44       1.00      1.00      1.00         1\n","          45       1.00      1.00      1.00         1\n","          46       0.25      1.00      0.40         1\n","           5       0.00      0.00      0.00         1\n","           6       0.00      0.00      0.00         1\n","           7       0.00      0.00      0.00         1\n","           8       0.00      0.00      0.00         1\n","           9       1.00      1.00      1.00         1\n","\n","    accuracy                           0.56        45\n","   macro avg       0.48      0.56      0.50        45\n","weighted avg       0.48      0.56      0.50        45\n","\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"ex79HOSBzQSN"},"execution_count":null,"outputs":[]}]}